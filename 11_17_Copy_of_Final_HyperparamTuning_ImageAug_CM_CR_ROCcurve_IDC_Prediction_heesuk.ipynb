{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heesukjang/Income_Prediction/blob/main/11_17_Copy_of_Final_HyperparamTuning_ImageAug_CM_CR_ROCcurve_IDC_Prediction_heesuk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FALL 2022<br>\n",
        "W207 Applied Machine Learning<br>\n",
        "Heesuk Jang\n",
        " \n",
        "\n",
        "**XGBoost:** an optimized version of Gradient Boosting / much more evolved version of Random Forest in terms of speed and accuracy\n",
        "\n",
        "#Predicting IDC with Breast Histopathology Images using CNN\n",
        "\n"
      ],
      "metadata": {
        "id": "5DebDWCL0KeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SRkZHKoWswZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33339e14-86e5-4d3d-babc-77ba09a18694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import joblib\n",
        "import glob\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import *                            # confusion_matrix, log_loss, accuracy_score\n",
        "from sklearn.model_selection import *                    # train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import *  \n",
        "# from sklearn.ensemble import *\n",
        "from sklearn.svm import *\n",
        "from sklearn.linear_model import *                       # LinearRegression\n",
        "from sklearn.discriminant_analysis import *\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import metrics\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import RandomFlip, RandomZoom, RandomRotation, Conv2D, MaxPooling2D, AveragePooling2D, Input, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad, RMSprop\n",
        "from keras.layers import ReLU, LeakyReLU\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.layers import ReLU, LeakyReLU\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, auc\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "tf.get_logger().setLevel('INFO')\n",
        "\n",
        "import cv2 as cv\n",
        "import skimage.io as io\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Required to read the data from Kaggle\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Optuna and Version Check"
      ],
      "metadata": {
        "id": "2NlEwp6jmZsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet optuna\n",
        "# import optuna\n",
        "# optuna.__version__"
      ],
      "metadata": {
        "id": "vbF2pFtlbJaH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import keras_tuner as kt\n",
        "\n",
        "# !pip install xgboost\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "XmhJR4rTcSlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73269dd-2f3e-4a3a-acf5-be3ef5a0750e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 36.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 55.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling and testing the GPU"
      ],
      "metadata": {
        "id": "2JC1sfIYmRuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "lpo1_3kNlzoo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling and testing the TPU"
      ],
      "metadata": {
        "id": "s3LsW0cGnF8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observe TensorFlow speedup on GPU relative to CPU"
      ],
      "metadata": {
        "id": "Xqgm8yAcmT7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip gdrive/MyDrive/Kaggle/CNN_IDC/Dataset.zip\n",
        "\n",
        "#replace these paths with the paths of your \n",
        "val_image_directory = '/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Validate'\n",
        "train_image_directory = '/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Train'\n",
        "test_image_directory = '/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Test'\n",
        "directory_path = '/content/gdrive/MyDrive/Kaggle/CNN_IDC'"
      ],
      "metadata": {
        "id": "uyWJuOkZuCVl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paths(directory):\n",
        "  all_path = []\n",
        "  idc_image_path = []\n",
        "  idc_image_label = []\n",
        "\n",
        "  for dir, subdir, files in os.walk(directory):\n",
        "    path = dir + \"/\"\n",
        "    all_path.append(path)\n",
        "\n",
        "  for i in range(len(all_path)):\n",
        "    for file in os.listdir(all_path[i]):\n",
        "      test = file\n",
        "      path = all_path[i] + test\n",
        "      if path.lower().endswith('.png'):\n",
        "        idc_image_path.append(path)\n",
        "\n",
        "  for i in range(len(idc_image_path)):\n",
        "    split_test = idc_image_path[i]\n",
        "    split_path = split_test.split(\"/\")\n",
        "    directory_name = split_path[7]\n",
        "    idc_image_label.append('class_' + split_path[8])\n",
        "  return idc_image_path, idc_image_label, directory_name"
      ],
      "metadata": {
        "id": "N892xh1IM4q7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths, train_labels, train_dir = get_paths(train_image_directory)\n",
        "val_paths, val_labels, val_dir = get_paths(val_image_directory)\n",
        "test_paths, test_labels, test_dir = get_paths(test_image_directory)"
      ],
      "metadata": {
        "id": "SJ6Cl4wtmxjO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels[:5])\n",
        "print(train_labels[-5:])\n",
        "\n",
        "print(len(train_paths), len(train_labels))\n",
        "print(len(test_paths), len(test_labels))\n",
        "print(len(val_paths), len(val_labels))"
      ],
      "metadata": {
        "id": "NIf9ETAsmxa2",
        "outputId": "7f1b4e60-60bf-4d80-be16-eed19b28702f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['class_0', 'class_0', 'class_0', 'class_0', 'class_0']\n",
            "['class_1', 'class_1', 'class_1', 'class_1', 'class_1']\n",
            "800 800\n",
            "200 200\n",
            "200 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_paths[:2])\n",
        "print(train_labels[:10])\n",
        "print(train_dir)"
      ],
      "metadata": {
        "id": "uCJzEwS8mxSR",
        "outputId": "202fa1ae-8a3f-4769-b587-1c1ad901aefa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Train/0/12880_idx5_x451_y701_class0.png', '/content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset/Train/0/9345_idx5_x2001_y2001_class0.png']\n",
            "['class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0', 'class_0']\n",
            "Train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataframes(idc_image_path, idc_image_label, directory_name):\n",
        "  same_name = directory_name.lower() + '_'\n",
        "  #creating the dataframes that we will be passing to our generators\n",
        "  idc_data_cleaned = {'path': idc_image_path,\n",
        "            'label': idc_image_label}\n",
        "  idc_df = pd.DataFrame(idc_data_cleaned)\n",
        "  idc_df['label_int'] = idc_df['label'].str.split(\"_\", expand=True)[1]    # Added a new column 'label_int'\n",
        "  df = idc_df.sample(frac = 1)\n",
        "  print(df)\n",
        "  csv_path = directory_path\n",
        "  csv_file = df.to_csv(csv_path + '/' + same_name + 'idc_dataframe.csv')\n",
        "  csv_file_path = csv_path + '/' + same_name + 'idc_dataframe.csv'\n",
        "  return csv_file_path"
      ],
      "metadata": {
        "id": "_WU9qt2RmxHZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe = create_dataframes(train_paths, train_labels, train_dir)\n",
        "print('type(train_dataframe): ',type(train_dataframe))\n",
        "train_dataframe"
      ],
      "metadata": {
        "id": "MJxUgm39NeU7",
        "outputId": "851e71dd-bd03-411f-cf75-2d8f12cfff5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  path    label label_int\n",
            "534  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "598  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "258  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "505  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "131  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "..                                                 ...      ...       ...\n",
            "480  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "374  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "718  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "496  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "657  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "\n",
            "[800 rows x 3 columns]\n",
            "type(train_dataframe):  <class 'str'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Kaggle/CNN_IDC/train_idc_dataframe.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe = create_dataframes(train_paths, train_labels, train_dir)\n",
        "train_generator = pd.read_csv(train_dataframe)\n",
        "\n",
        "test_dataframe = create_dataframes(test_paths, test_labels, test_dir)\n",
        "test_generator = pd.read_csv(test_dataframe)\n",
        "\n",
        "val_dataframe = create_dataframes(val_paths, val_labels, val_dir)\n",
        "val_generator = pd.read_csv(val_dataframe)"
      ],
      "metadata": {
        "id": "hnDu_3N4mw1G",
        "outputId": "4bca811f-7554-4f7a-e8ad-9d6c65f321d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  path    label label_int\n",
            "542  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "706  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "729  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "241  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "205  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "..                                                 ...      ...       ...\n",
            "425  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "208  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "191  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "128  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "225  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "\n",
            "[800 rows x 3 columns]\n",
            "                                                  path    label label_int\n",
            "6    /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "117  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "123  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "34   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "48   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "..                                                 ...      ...       ...\n",
            "180  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "171  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "26   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "192  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "89   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "\n",
            "[200 rows x 3 columns]\n",
            "                                                  path    label label_int\n",
            "154  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "152  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "115  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "166  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "86   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "..                                                 ...      ...       ...\n",
            "43   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "189  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "31   /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "7    /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_0         0\n",
            "169  /content/gdrive/MyDrive/Kaggle/CNN_IDC/Dataset...  class_1         1\n",
            "\n",
            "[200 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # import tqdm\n",
        "\n",
        "# Apply gray scale to all images, flatten and store array / shape in new columns\n",
        "def get_img_arrays(df,):\n",
        "    # read each image array from corresponding path as grayscale and flatten the image array\n",
        "    df['img_array'] = df.progress_apply(lambda x : io.imread(x['path'],as_gray=True).flatten(),axis=1) # make sure to specify axis = 1\n",
        "    # get the shape of each image array and store it in the dataframe\n",
        "    df['array_shape'] = df.progress_apply(lambda x : x['img_array'].shape[0],axis=1) # make sure to specify axis = 1\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "nkWYjNvnHrrn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # import tqdm\n",
        "tqdm.pandas() # initialize tqdm for pandas\n",
        "\n",
        "# # tqdm is a library that enables you to visualize the progress of a for loop by displaying a configurable progress bar\n",
        "\n",
        "train_generator = get_img_arrays(df = train_generator)\n",
        "val_generator = get_img_arrays(df = val_generator)\n",
        "test_generator = get_img_arrays(df = test_generator)"
      ],
      "metadata": {
        "id": "d9iPVxRZHreQ",
        "outputId": "3a6a281c-d899-47a6-e6f6-0475bfb084f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 249/800 [01:45<04:57,  1.85it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.array_shape.value_counts())\n",
        "# print(val_generator.array_shape.value_counts())\n",
        "# print(test_generator.array_shape.value_counts())"
      ],
      "metadata": {
        "id": "wAv7RXmVQX4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop these images as they add unnecessary noise to our model\n",
        "train_weird_imgs = train_generator[train_generator['array_shape'] != 2500]\n",
        "val_weird_imgs = val_generator[val_generator['array_shape'] != 2500]\n",
        "test_weird_imgs = test_generator[test_generator['array_shape'] != 2500]\n",
        "\n",
        "\n",
        "weird_imgs = train_weird_imgs.append(val_weird_imgs)\n",
        "weird_imgs = weird_imgs.append(test_weird_imgs)\n",
        "weird_imgs['dataset'] = weird_imgs['path'].str.split('/', expand=True)[7]\n",
        "weird_imgs.reset_index(drop=True)\n",
        "\n",
        "train_generator.drop(train_weird_imgs.index,inplace=True)\n",
        "val_generator.drop(val_weird_imgs.index,inplace=True)\n",
        "test_generator.drop(test_weird_imgs.index,inplace=True)\n",
        "\n",
        "# print(len(weird_imgs))\n",
        "# print(len(train_generator))\n",
        "# print(len(val_generator))\n",
        "# print(len(test_generator))\n",
        "# print(train_generator.columns)\n",
        "# val_generator.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "gpOS9_alHrTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import skimage.io as io\n",
        "\n",
        "def display_images(subclass):\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20,8))\n",
        "  for idx, ax in enumerate(axes.flat):\n",
        "    image_wo_path = os.path.basename(subclass.path[idx])\n",
        "    # print(image_wo_path)\n",
        "    subtitle = 'Class ' + image_wo_path.rsplit('.')[0][-1] + ': ' + subclass.dataset[idx]\n",
        "    img = io.imread(subclass.path[idx])\n",
        "    ax.imshow(img)\n",
        "    # ax.axis('off')\n",
        "    ax.set_title(subtitle, size=14)   \n",
        "  fig.tight_layout() \n",
        "  plt.show() \n",
        "\n",
        "print()\n",
        "display_images(weird_imgs.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "x9euSsBNHrIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data using ImageDataGenerator"
      ],
      "metadata": {
        "id": "iHdf3Jd0ipmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://faroit.com/keras-docs/0.3.3/preprocessing/image/\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "\n",
        "def data_generator():\n",
        "\n",
        "  with_aug_datagen = ImageDataGenerator(\n",
        "      featurewise_center = True,                     # transforms the images to 0 mean\n",
        "      featurewise_std_normalization = True,          # divide inputs by std of the dataset\n",
        "      # zca_epsilon=1e-06,\n",
        "      rotation_range=30,                      # randomly rotate image by 10 degrees\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      brightness_range=[0.4, 1.5],\n",
        "      shear_range=0.2,                        # distort image along an axis mostly to create or recify the perception angles     \n",
        "      zoom_range=0.2,                         # zomming image: zoom_range > 1 => zoom out, zoom_range < 1 => zoom in                  \n",
        "      fill_mode='nearest',                    # when the image is rotated, some pixels will move outside the image and leave an empty area that needs to be filled in, 'nearest': simply replace the empty area with the nearest spectral values.\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True)\n",
        "      \n",
        "  without_aug_datagen = ImageDataGenerator()\n",
        "  \n",
        "# =================================================================\n",
        "  # if with_augmented_images:\n",
        "  train_data_generator = with_aug_datagen.flow_from_dataframe(\n",
        "        train_generator,\n",
        "        directory = None,\n",
        "        x_col =  'path',\n",
        "        y_col =  'label',\n",
        "        weight_col=None,\n",
        "        target_size=(50,50),\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode=\"categorical\",\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        seed=1234\n",
        "        # validate_filenames=True\n",
        "    )\n",
        "  # else:\n",
        "  #   train_data_generator = without_aug_datagen.flow_from_dataframe(\n",
        "  #       train_generator,\n",
        "  #       directory = None,\n",
        "  #       x_col =  'path',\n",
        "  #       y_col =  'label',\n",
        "  #       weight_col=None,\n",
        "  #       target_size=(50,50),\n",
        "  #       color_mode=\"grayscale\",\n",
        "  #       class_mode=\"categorical\",\n",
        "  #       batch_size=32,\n",
        "  #       shuffle=True,\n",
        "  #       seed=1234\n",
        "  #       # validate_filenames=True\n",
        "  #   )\n",
        "\n",
        "  validation_data_generator = without_aug_datagen.flow_from_dataframe(\n",
        "      val_generator,\n",
        "      directory = None,\n",
        "      x_col =  'path',\n",
        "      y_col =  'label',\n",
        "      weight_col=None,\n",
        "      # target_size=(hp_target_size, hp_target_size),\n",
        "      target_size=(50,50),\n",
        "      color_mode=\"grayscale\",\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=32,\n",
        "      shuffle=True,\n",
        "      seed=1234\n",
        "      # validate_filenames=True\n",
        "  )\n",
        "\n",
        "  test_data_generator = without_aug_datagen.flow_from_dataframe(\n",
        "      test_generator,\n",
        "      directory = None,\n",
        "      x_col =  'path',\n",
        "      y_col =  'label',\n",
        "      weight_col=None,\n",
        "      target_size=(50,50),\n",
        "      color_mode=\"grayscale\",\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=32,\n",
        "      shuffle=True,              # Kesha set to shuffle=True but we don't want to shuffle our testing data around, which it does so by default\n",
        "      seed=1234\n",
        "      # validate_filenames=True\n",
        "  )\n",
        "  return train_data_generator, validation_data_generator, test_data_generator\n",
        "\n",
        "# train_data_generator, validation_data_generator, test_data_generator = data_generator(with_augmented_images=True)\n",
        "train_data_generator, validation_data_generator, test_data_generator = data_generator()"
      ],
      "metadata": {
        "id": "9_IibHAwxE6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build CNN Model and Hyperparameter Tuning\n",
        "- **ReduceLROnPlateau**: A scheduling technique that monitors a particular quantity and decays the learning rate when the quantity is stop improving.\n",
        "- **ModelCheckpoint**: A sch\n",
        "- **BatchNormalization**: A feature that we add between the layers of neural network and it continuously takes the output from the previous layer and normalizes it before sending it to the next layer thereby helping stablizing the NN"
      ],
      "metadata": {
        "id": "I19l1YOYsfOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img_height = 50\n",
        "# img_width = 50\n",
        "# img_channel = 1\n",
        "# input_shape = (img_height, img_width, img_channel)"
      ],
      "metadata": {
        "id": "OQcHdDdYJnol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import ReLU, LeakyReLU\n",
        "# https://keras.io/guides/keras_tuner/getting_started/\n",
        "\n",
        "def build_cnn(hp): \n",
        "  # train_data_generator, validation_data_generator, test_data_generator = data_generator(hp)\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "# Define ranges of hyperparam values \n",
        "  # hp_img_size = hp.get('hp_target_size')\n",
        "  # hp_img_size = hp.Int('img_size', 50,96,120)\n",
        "  # hp_img_size = hp.Int('img_size', 50,96)\n",
        "  hp_kernel_size = hp.Int('kernel_size', min_value=2, max_value=5, step=1)\n",
        "  hp_strides = hp.Int('strides', min_value=1, max_value=2, step=1)\n",
        "  hp_pool_size = hp.Int('pool_size', min_value=2, max_value=3, step=1)\n",
        "  hp_activation = hp.Choice('activation', values=['tanh', 'relu', 'softmax', 'sigmoid', 'leaky_relu', 'elu', 'gelu','selu','swish'], default='relu')\n",
        "  hp_optimizer = hp.Choice('optimizer', values=['adadelta', 'adagrad', 'adam', 'rmsprop', 'sgd'], default='adam')\n",
        "  hp_filters_1 = hp.Int('filters_1', min_value=8, max_value=64, step=8)\n",
        "  hp_filters_2 = hp.Int('filters_2', min_value=8, max_value=128, step=16)\n",
        "  hp_filters_3 = hp.Int('filters_3', min_value=32, max_value=128, step=16)\n",
        "  hp_dense = hp.Int('dense_units', min_value=32, max_value=512, step=34)\n",
        "  hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-2)\n",
        "  hp_dropout_1 = hp.Float('dropout_rate_1', min_value=0.1, max_value=0.5, default=0.1, step=0.1)\n",
        "  hp_dropout_2 = hp.Float('dropout_rate_2', min_value=0.3, max_value=0.5, default=0.25, step=0.1)\n",
        "  hp_dropout_3 = hp.Float('dropout_rate_3', min_value=0.3, max_value=0.5, default=0.35, step=0.1)\n",
        "  hp_reduction_type = hp.Choice('reduction_type', values=['global_avg_pooling2d', 'max_pooling2d'])\n",
        "\n",
        "  # Define the model\n",
        "  model = tf.keras.Sequential()\n",
        "  # print('input_shape: ', (hp_img_size, hp_img_size, 1))\n",
        "  # 1st set of neural network layers (Input layer)\n",
        "  model.add(Conv2D(filters=hp_filters_1, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation, input_shape = (50,50,1)))  \n",
        "  # model.add(Conv2D(filters=hp_filters_1, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation, input_shape = (hp_img_size, hp_img_size, 1)))  \n",
        "  if hp_reduction_type == 'global_avg_pooling2d':\n",
        "    model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "  else:\n",
        "    model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides,hp_strides)))  \n",
        "  # if hp.Boolean('dropout_1'):\n",
        "    # model.add(Dropout(hp_dropout_1))\n",
        "  \n",
        "  # 2nd set of neural network layers\n",
        "  # if hp.Boolean('conv_layer_2'):  \n",
        "  model.add(Conv2D(filters=hp_filters_2, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation))\n",
        "  # if hp.Boolean('BatchNormalization_1'):\n",
        "  model.add(BatchNormalization())\n",
        "  if hp_reduction_type == 'global_avg_pooling2d':\n",
        "    model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "  else:\n",
        "    model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides,hp_strides)))  \n",
        "  # if hp.Boolean('dropout_2'):\n",
        "    # model.add(Dropout(hp_dropout_1))\n",
        "\n",
        "  # 3rd set of neural network layers\n",
        "  # if hp.Boolean('conv_layer_3'):  \n",
        "  model.add(Conv2D(filters=hp_filters_3, kernel_size=(hp_kernel_size,hp_kernel_size), padding='same', activation=hp_activation.lower()))\n",
        "  # if hp.Boolean('BatchNormalization_2'):\n",
        "  model.add(BatchNormalization())\n",
        "  if hp_reduction_type == 'global_avg_pooling2d':\n",
        "    model.add(GlobalAveragePooling2D(keepdims=True))\n",
        "  else:\n",
        "    model.add(MaxPooling2D(pool_size=(hp_pool_size,hp_pool_size), strides=(hp_strides, hp_strides)))  \n",
        "  # if hp.Boolean('dropout_3'):\n",
        "  # model.add(Dropout(hp_dropout_2))\n",
        "\n",
        "  # Flatten layer\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Fully connected dense layer\n",
        "  model.add(Dense(units = hp_dense, activation = hp_activation))\n",
        "  # if hp.Boolean('BatchNormalization_3'):\n",
        "  model.add(BatchNormalization())\n",
        "  if hp.Boolean('dropout_4'):\n",
        "    model.add(Dropout(hp_dropout_3))\n",
        "  \n",
        "  # Output layer\n",
        "  model.add(Dense(units = 2, activation = 'softmax'))        # output layer\n",
        "  \n",
        "  # Define the optimizer\n",
        "  def selected_optimizer(optimizer):\n",
        "    if optimizer.lower() == 'sgd':\n",
        "        return SGD(learning_rate=hp_learning_rate)           # SGD(learning_rate=learning_rate, momentum=0.95, decay=1, nesterov=True)\n",
        "    elif optimizer.lower() == 'adam':\n",
        "        return Adam(learning_rate=hp_learning_rate)          # Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8)\n",
        "    elif optimizer.lower() == 'adadelta':\n",
        "        return Adadelta(learning_rate=hp_learning_rate)      # Adadelta(learning_rate=learning_rate, rho=0.95, epsilon=1e-6)\n",
        "    elif optimizer.lower() == 'adagrad':\n",
        "        return Adagrad(learning_rate=hp_learning_rate)       # Adagrad(learning_rate=learning_rate, epsilon=1e-6)\n",
        "    elif optimizer.lower() == 'rmsprop':\n",
        "        return RMSprop(learning_rate=hp_learning_rate)       # RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-6)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=CategoricalCrossentropy(), \n",
        "                optimizer=selected_optimizer(hp_optimizer), \n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "-b-MQQn9Erwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the **max_trials** variable represents the number of hyperparameter combinations that will be tested by the tuner, while the **execution_per_trial** variable is the number of models that should be built and fit for each trial for robustness purposes. The next section explains how to set them\n",
        "\n",
        "https://www.sicara.fr/blog-technique/hyperparameter-tuning-keras-tuner<br>\n",
        "https://www.kaggle.com/code/fchollet/keras-kerastuner-best-practices/notebook"
      ],
      "metadata": {
        "id": "SnHwzogJu0BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "stop_early = EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience = 3)\n",
        "# stop_early = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 3)\n",
        "# 1) Hyperband\n",
        "# tuner = kt.Hyperband(build_cnn,                     \n",
        "#                      objective='val_accuracy',\n",
        "#                      max_epochs=10,\n",
        "#                      factor=3,\n",
        "#                      overwrite=True,\n",
        "#                      directory='hj_dir',\n",
        "#                      project_name='breast_cancer_classification')\n",
        "# 2) RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "          # CNN_HyperModel(),\n",
        "          build_cnn,\n",
        "          objective=\"val_accuracy\",\n",
        "          max_trials=100,                     # max_trials=50 or 10,\n",
        "          executions_per_trial=2,           # executions_per_trial=10,\n",
        "          overwrite=True,\n",
        "          directory=\"hj_dir\",\n",
        "          project_name=\"breast_cancer_classification\")\n",
        "\n",
        "tuner.search(train_data_generator, \n",
        "             epochs=10, \n",
        "             callbacks=[stop_early], \n",
        "             validation_data=validation_data_generator)"
      ],
      "metadata": {
        "id": "yH0jfZqX2Qxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_all = np.concatenate((train_data_generator, validation_data_generator))"
      ],
      "metadata": {
        "id": "4-ChFsRGFKYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/"
      ],
      "metadata": {
        "id": "5L2ndY3cZWSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]     # num_trials=5\n",
        "# model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters()[0]     # num_trials=5\n",
        "model = build_cnn(best_hps)\n",
        "\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, classification_report\n",
        "# from sklearn import svm\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, min_delta=0.0001, patience=1, verbose=1)\n",
        "\n",
        "file_path = 'weights.hdf5'    # save the weights and biases\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Fit with the entire dataset (both training and validation sets)\n",
        "\n",
        "history = model.fit(train_data_generator,\n",
        "                  epochs=10,\n",
        "                  # steps_per_epoch=len(train_data_generator)//BATCH_SIZE, \n",
        "                  callbacks=[lr_reduce, checkpoint, stop_early],\n",
        "                  validation_data = validation_data_generator)\n",
        "                  # validation_steps=len(validation_data_generator)//BATCH_SIZE)\n",
        "# history = model.fit_generator(train_data_generator.flow(train_data_generator, batch_size=BATCH_SIZE),\n",
        "#                   steps_per_epoch = train_generator.shape[0] / BATCH_SIZE,\n",
        "#                   epochs=60,\n",
        "#                   callbacks=[lr_reduce, checkpoint, stop_early],\n",
        "#                   validation_data = validation_data_generator)\n",
        "# history = model.fit_generator(train_data_generator,\n",
        "#                   epochs=60,\n",
        "#                   callbacks=[lr_reduce, checkpoint, stop_early],\n",
        "#                   validation_data = validation_data_generator).decision_function(test_data_generator)\n",
        "# pd.DataFrame(history.history)\n",
        "\n",
        "# Plot loss and accuracy of best model on every epoch\n",
        "hist = history.history\n",
        "x_arr = np.arange(len(hist['loss'])) + 1\n",
        "\n",
        "fig = plt.figure(figsize=(16,6))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "ax.plot(x_arr, hist['loss'], '-o', label='Train Loss')\n",
        "ax.plot(x_arr, hist['val_loss'], '--<', label='Validation Loss')\n",
        "ax.legend(fontsize=12)\n",
        "ax.set_xlabel('Epoch', size=14)\n",
        "ax.set_ylabel('Loss', size=14)\n",
        "ax.set_title('Loss', size=20)\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "ax.plot(x_arr, hist['accuracy'], '-o', label='Train Acc.')\n",
        "ax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation Acc.')\n",
        "ax.legend(fontsize=12)\n",
        "ax.set_xlabel('Epoch', size=14)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "ax.set_title('Accuracy', size=20);\n",
        "\n",
        "test_accuracy = round(model.evaluate(test_data_generator, verbose=0,\n",
        "                          return_dict=True)['accuracy'], 2)\n",
        "training_accuracy = round(history.history['accuracy'][-1], 2)\n",
        "val_accuracy = round(history.history['val_accuracy'][-1], 2)\n",
        "count_params = model.count_params()\n",
        "model_summary = pd.DataFrame({'test_acc':test_accuracy,\n",
        "                              'training_acc': training_accuracy,\n",
        "                              'val_acc': val_accuracy,\n",
        "                              'num_params':  f'{count_params:,}'}, index=[0]) \n",
        "\n",
        "print(model.summary())\n",
        "model_summary"
      ],
      "metadata": {
        "id": "iAgUf5GbSBbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the hyperparameter values of top 2 models \n",
        "tuner.results_summary(1)"
      ],
      "metadata": {
        "id": "zTTakeLM0-PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, classification_report\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "\n",
        "def error_analysis(model_name, model, test_labels, test_data_generator):\n",
        "  num_of_test_samples = len(test_labels)\n",
        "  classes = ['Non-IDC(0)','IDC(1)']\n",
        "\n",
        "  y_true = test_data_generator.classes\n",
        "  y_pred = model.predict_generator(test_data_generator, num_of_test_samples // BATCH_SIZE + 1)    # probabilities\n",
        "  y_pred_argmax = np.argmax(y_pred, axis=1)                      # 0s and 1s: return the indicies of the max values along the axis (axis=1: each row)   TODO: look into prob threshold\n",
        "\n",
        "\n",
        "  # Confusion matrix  \n",
        "  conf_max = confusion_matrix(y_true, y_pred_argmax)             # conf.max should use 0s and 1s for y_pred\n",
        "  perf_conf_max = conf_max.astype('float')/conf_max.sum(axis=1)[:np.newaxis]*100\n",
        "  df_perf_conf_max = pd.DataFrame(perf_conf_max, index=classes, columns=classes)\n",
        "\n",
        "  plt.figure(figsize=(6,5))\n",
        "  sns.heatmap(df_perf_conf_max, annot=True, cmap='coolwarm', annot_kws={'fontsize':16}, linewidth=0.5, fmt='.0f')  \n",
        "  plt.xlabel('Predicted Label', fontsize=14)\n",
        "  plt.ylabel('True Label', fontsize=14)\n",
        "  plt.title('Confusion Matrix (%)', fontsize=15)\n",
        "  \n",
        "  # Classification report (classification_report should use 0s and 1s for y_pred)\n",
        "  print('=============== Classification Report ===============\\n\\n', classification_report(y_true, y_pred_argmax, target_names=['Non-IDC', 'IDC']), '\\n=====================================================\\n')\n",
        "  \n",
        "  # Cross validation score\n",
        "  # cv = ShuffleSplit(n_splits=100, test_size=.25, random_state=0)\n",
        "  # cv_score = np.median(cross_val_score(model, y_true, y_pred, cv=cv))*100\n",
        "  # print('cv_score = ',cv_score)\n",
        "\n",
        "  # Precision, recall, and f1_score\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred_argmax).ravel()     # np.ravel(): returns contiguous flattened array (1D array with all the input-array elements and with the same type as it)\n",
        "  accuracy = round(accuracy_score(y_true, y_pred_argmax), 2)\n",
        "  precision = round(precision_score(y_true, y_pred_argmax), 2)\n",
        "  recall = round(recall_score(y_true, y_pred_argmax), 2)\n",
        "  f1score = round((2*precision*recall)/(precision+recall), 2)\n",
        "  error_rate = round(1-accuracy, 2)\n",
        "\n",
        "  # cohen_kappa score and zero_one loss\n",
        "  cohen_kappa = round(cohen_kappa_score(y_true, y_pred_argmax), 2)\n",
        "  zo_loss = round(zero_one_loss(y_true, y_pred_argmax), 2)\n",
        "\n",
        "  # Area under the ROC curve\n",
        "  roc_log = roc_auc_score(y_true, y_pred[:,1], multi_class='ovr')   # for the roc curve, we need to use a vector of probabilities so just chose one column and all rows\n",
        "  fpr, tpr, threshold = roc_curve(y_true, y_pred[:,1])\n",
        "  area_under_curve = round(auc(fpr, tpr), 2)  \n",
        "\n",
        "  plt.figure(figsize=(6,5))\n",
        "  plt.plot([0, 1], [0, 1], 'r--')  \n",
        "  plt.plot(fpr, tpr, label='ROC-AUC = {:.2f}'.format(area_under_curve))  \n",
        "  plt.xlabel('False positive rate', fontsize=14)\n",
        "  plt.ylabel('True positive rate', fontsize=14)\n",
        "  plt.title('ROC Curve', fontsize=18)\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  model_summary = pd.DataFrame({'Model':model_name,\n",
        "                                'Accuracy': accuracy,\n",
        "                                'Precision': precision,\n",
        "                                'Recall': recall,\n",
        "                                'F1_score': f1score,\n",
        "                                'Error Rate': error_rate,\n",
        "                                'ROC-AUC score': area_under_curve,\n",
        "                                'Cohen Kappa': cohen_kappa,\n",
        "                                'Zero-One Loss': zo_loss}, index=[0]) \n",
        "  return model_summary\n",
        "  \n",
        "error_analysis('CNN', model, test_labels, test_data_generator)"
      ],
      "metadata": {
        "id": "55uBtQIKC7fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTqd7IJeTaVT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}